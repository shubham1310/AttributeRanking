6
6
6
6
6
6
6
6
6
6
8
8
8
8
8
8
8
8
8
8
8
7
7
7
7
7
7
7
7
7
7
7
7
5
5
5
5
5
5
5
5
5
2
2
2
2
2
2
2
2
2
2
2
2
1
1
1
1
1
1
1
1
1
1
1
1
4
4
4
4
4
4
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
3
3
3
3
3
(680, 680)
load dataset
('The number of data, train:', 646, 'validate:', 34)
prepare initialized model!
step: 100
NDCG@100 | train: 0.547032772513, test: 0.716780180707
step: 200
NDCG@100 | train: 0.577181103976, test: 0.861504973339
step: 300
NDCG@100 | train: 0.559795707104, test: 0.701470649401
step: 400
NDCG@100 | train: 0.552864360587, test: 0.819444458489
step: 500
NDCG@100 | train: 0.565151642445, test: 0.81823297078
step: 600
NDCG@100 | train: 0.700898575464, test: 0.83516094742
step: 700
NDCG@100 | train: 0.704685676599, test: 0.741875551897
step: 800
NDCG@100 | train: 0.799864937183, test: 0.762572004705
step: 900
NDCG@100 | train: 0.58010042432, test: 0.636121415882
step: 1000
NDCG@100 | train: 0.552383125926, test: 0.875487982542
step: 1100
NDCG@100 | train: 0.70838979677, test: 0.821205613191
step: 1200
NDCG@100 | train: 0.684984453134, test: 0.780902410791
step: 1300
NDCG@100 | train: 0.775257038409, test: 0.844791100089
step: 1400
NDCG@100 | train: 0.765627099025, test: 0.829190809058
step: 1500
NDCG@100 | train: 0.770041749314, test: 0.871422036189
step: 1600
NDCG@100 | train: 0.668651380189, test: 0.844652861122
step: 1700
NDCG@100 | train: 0.722107887253, test: 0.867383066219
step: 1800
NDCG@100 | train: 0.344626155711, test: 0.715202761866
step: 1900
NDCG@100 | train: 0.742508825242, test: 0.875241560844
step: 2000
NDCG@100 | train: 0.779923156205, test: 0.85027866802
step: 2100
NDCG@100 | train: 0.722342880889, test: 0.840782889656
step: 2200
NDCG@100 | train: 0.746711205523, test: 0.907948717228
step: 2300
NDCG@100 | train: 0.73460658933, test: 0.793201779866
step: 2400
NDCG@100 | train: 0.366268441622, test: 0.62946298108
step: 2500
NDCG@100 | train: 0.763358022551, test: 0.845265898991
step: 2600
NDCG@100 | train: 0.789725255889, test: 0.855478040717
step: 2700
NDCG@100 | train: 0.760819027637, test: 0.880899198978
step: 2800
NDCG@100 | train: 0.740554807654, test: 0.827975837835
step: 2900
NDCG@100 | train: 0.786650495314, test: 0.780608174545
step: 3000
NDCG@100 | train: 0.828187150164, test: 0.796433500705
step: 3100
NDCG@100 | train: 0.787131596833, test: 0.748606929594
step: 3200
NDCG@100 | train: 0.865453672444, test: 0.775383295952
step: 3300
NDCG@100 | train: 0.815633846916, test: 0.825032579826
step: 3400
NDCG@100 | train: 0.868830226073, test: 0.828845164964
step: 3500
NDCG@100 | train: 0.840841162714, test: 0.834600270344
step: 3600
NDCG@100 | train: 0.830031133548, test: 0.823025652833
step: 3700
NDCG@100 | train: 0.844068876506, test: 0.796574064993
step: 3800
NDCG@100 | train: 0.84448294701, test: 0.791304854254
step: 3900
NDCG@100 | train: 0.877132339788, test: 0.752120030856
step: 4000
NDCG@100 | train: 0.858215485077, test: 0.794919023916
step: 4100
NDCG@100 | train: 0.870747781877, test: 0.779964716
step: 4200
NDCG@100 | train: 0.788675477502, test: 0.815986406162
step: 4300
NDCG@100 | train: 0.841894918537, test: 0.778848953704
step: 4400
NDCG@100 | train: 0.847462693776, test: 0.783646092673
step: 4500
NDCG@100 | train: 0.85093649951, test: 0.728254704574
step: 4600
NDCG@100 | train: 0.816982051106, test: 0.830998132744
step: 4700
NDCG@100 | train: 0.845569531574, test: 0.856053807436
step: 4800
NDCG@100 | train: 0.81631830177, test: 0.886488614562
step: 4900
NDCG@100 | train: 0.806341345711, test: 0.876882289393
step: 5000
NDCG@100 | train: 0.863121687405, test: 0.84242476537
step: 5100
NDCG@100 | train: 0.848568085476, test: 0.832933967838
step: 5200
NDCG@100 | train: 0.820615519664, test: 0.742912557663
step: 5300
NDCG@100 | train: 0.822948613793, test: 0.801356812243
step: 5400
NDCG@100 | train: 0.865215321071, test: 0.774427757682
step: 5500
NDCG@100 | train: 0.844413972166, test: 0.779337954491
step: 5600
NDCG@100 | train: 0.813317491918, test: 0.828920375578
step: 5700
NDCG@100 | train: 0.839669682036, test: 0.786415457031
step: 5800
NDCG@100 | train: 0.840870986631, test: 0.784851687262
step: 5900
NDCG@100 | train: 0.847974378171, test: 0.775261028979
step: 6000
NDCG@100 | train: 0.840426082367, test: 0.787813205755
step: 6100
NDCG@100 | train: 0.846636811908, test: 0.793510210001
step: 6200
NDCG@100 | train: 0.849787433449, test: 0.798362407533
step: 6300
NDCG@100 | train: 0.830257576036, test: 0.798112616402
step: 6400
NDCG@100 | train: 0.836029801907, test: 0.81274426241
step: 6500
NDCG@100 | train: 0.860240373981, test: 0.870985414726
step: 6600
NDCG@100 | train: 0.856416794412, test: 0.784901290894
step: 6700
NDCG@100 | train: 0.879817175433, test: 0.773606367495
step: 6800
NDCG@100 | train: 0.881915853076, test: 0.814598059287
step: 6900
NDCG@100 | train: 0.847420795189, test: 0.853538337647
step: 7000
NDCG@100 | train: 0.839486055364, test: 0.804198561884
step: 7100
NDCG@100 | train: 0.835349780995, test: 0.788616999413
step: 7200
NDCG@100 | train: 0.815743296478, test: 0.774444550211
step: 7300
NDCG@100 | train: 0.861678771182, test: 0.858721946283
step: 7400
NDCG@100 | train: 0.623542910112, test: 0.765283583972
step: 7500
NDCG@100 | train: 0.850907901611, test: 0.827688395751
step: 7600
NDCG@100 | train: 0.876206777997, test: 0.787968922839
step: 7700
NDCG@100 | train: 0.90445739671, test: 0.870196590698
step: 7800
NDCG@100 | train: 0.80095065721, test: 0.801137467994
step: 7900
NDCG@100 | train: 0.869045375174, test: 0.852613302258
step: 8000
NDCG@100 | train: 0.903277787267, test: 0.79818486173
step: 8100
NDCG@100 | train: 0.852152402018, test: 0.858330586781
step: 8200
NDCG@100 | train: 0.906391144351, test: 0.837432479666
step: 8300
NDCG@100 | train: 0.898624363019, test: 0.839561642451
step: 8400
NDCG@100 | train: 0.892838399197, test: 0.838258256415
step: 8500
NDCG@100 | train: 0.874767069405, test: 0.825797030198
step: 8600
NDCG@100 | train: 0.849615868056, test: 0.855429884107
step: 8700
NDCG@100 | train: 0.824453201336, test: 0.831986458243
step: 8800
NDCG@100 | train: 0.833627882801, test: 0.82998921761
step: 8900
NDCG@100 | train: 0.891486000569, test: 0.838758598407
step: 9000
NDCG@100 | train: 0.888268948923, test: 0.90880134602
step: 9100
NDCG@100 | train: 0.896972772229, test: 0.92023153948
step: 9200
NDCG@100 | train: 0.893255051796, test: 0.909433371818
step: 9300
NDCG@100 | train: 0.867782906618, test: 0.895068116497
step: 9400
NDCG@100 | train: 0.920775236117, test: 0.912321939258
step: 9500
NDCG@100 | train: 0.908705449194, test: 0.851614817817
step: 9600
NDCG@100 | train: 0.884904525658, test: 0.888744695306
step: 9700
NDCG@100 | train: 0.907346146897, test: 0.888975472951
step: 9800
NDCG@100 | train: 0.903105519984, test: 0.845900922202
step: 9900
NDCG@100 | train: 0.9026864868, test: 0.882349807716
step: 10000
NDCG@100 | train: 0.891447776066, test: 0.853778161876
save the model
save the optimizer
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
3
3
3
3
5
5
5
5
5
5
5
5
5
7
7
7
7
7
7
7
7
7
7
7
7
6
6
6
6
6
6
6
6
6
6
6
6
8
8
8
8
8
8
8
8
8
8
8
8
8
4
4
4
4
4
4
4
4
4
4
4
4
4
(680, 680)
load dataset
('The number of data, train:', 646, 'validate:', 34)
prepare initialized model!
step: 100
NDCG@100 | train: 0.408670435123, test: 0.633470153888
step: 200
NDCG@100 | train: 0.295327058356, test: 0.518515925526
step: 300
NDCG@100 | train: 0.295327058356, test: 0.518515925526
step: 400
NDCG@100 | train: 0.401232086939, test: 0.656558607104
step: 500
NDCG@100 | train: 0.42471592935, test: 0.781970191656
step: 600
NDCG@100 | train: 0.49413921221, test: 0.610234678588
step: 700
NDCG@100 | train: 0.570511510299, test: 0.807614708242
step: 800
NDCG@100 | train: 0.493612697378, test: 0.854808424043
step: 900
NDCG@100 | train: 0.473050895336, test: 0.746931476955
step: 1000
NDCG@100 | train: 0.43506779266, test: 0.518515925526
step: 1100
NDCG@100 | train: 0.532669354531, test: 0.826039106045
step: 1200
NDCG@100 | train: 0.58569555108, test: 0.657407699424
step: 1300
NDCG@100 | train: 0.642707124816, test: 0.83647450655
step: 1400
NDCG@100 | train: 0.387859992706, test: 0.612836945778
step: 1500
NDCG@100 | train: 0.361969281499, test: 0.518515925526
step: 1600
NDCG@100 | train: 0.611622597334, test: 0.612836945778
step: 1700
NDCG@100 | train: 0.478070062316, test: 0.754879146373
step: 1800
NDCG@100 | train: 0.554605485751, test: 0.790355119531
step: 1900
NDCG@100 | train: 0.276857871936, test: 0.518515925526
step: 2000
NDCG@100 | train: 0.557047610132, test: 0.807303415252
step: 2100
NDCG@100 | train: 0.705489328938, test: 0.86911674356
step: 2200
NDCG@100 | train: 0.71221136443, test: 0.716534577989
step: 2300
NDCG@100 | train: 0.524196818005, test: 0.695651197734
step: 2400
NDCG@100 | train: 0.671490986903, test: 0.816716339981
step: 2500
NDCG@100 | train: 0.685323898056, test: 0.801812585209
step: 2600
NDCG@100 | train: 0.819906708941, test: 0.783802431952
step: 2700
NDCG@100 | train: 0.666192505896, test: 0.871465979306
step: 2800
NDCG@100 | train: 0.758192919053, test: 0.930229771493
step: 2900
NDCG@100 | train: 0.608909651853, test: 0.780362491474
step: 3000
NDCG@100 | train: 0.729844846031, test: 0.850458967551
step: 3100
NDCG@100 | train: 0.706389616586, test: 0.814711171917
step: 3200
NDCG@100 | train: 0.354786212867, test: 0.612836945778
step: 3300
NDCG@100 | train: 0.73752177326, test: 0.869520099777
step: 3400
NDCG@100 | train: 0.664251253131, test: 0.702209647744
step: 3500
NDCG@100 | train: 0.63116755777, test: 0.702573329329
step: 3600
NDCG@100 | train: 0.783397171334, test: 0.742478607016
step: 3700
NDCG@100 | train: 0.878050599701, test: 0.911532895276
step: 3800
NDCG@100 | train: 0.808070730767, test: 0.931877908512
step: 3900
NDCG@100 | train: 0.719419260254, test: 0.895428215419
step: 4000
NDCG@100 | train: 0.671977589189, test: 0.771295096893
step: 4100
NDCG@100 | train: 0.777333189627, test: 0.842933896936
step: 4200
NDCG@100 | train: 0.797718397875, test: 0.802205560313
step: 4300
NDCG@100 | train: 0.770003485102, test: 0.788062326559
step: 4400
NDCG@100 | train: 0.87035349565, test: 0.917139274606
step: 4500
NDCG@100 | train: 0.683777363761, test: 0.760230850088
step: 4600
NDCG@100 | train: 0.794279790907, test: 0.814785784746
step: 4700
NDCG@100 | train: 0.746222529505, test: 0.709856542216
step: 4800
NDCG@100 | train: 0.721372563572, test: 0.811241965215
step: 4900
NDCG@100 | train: 0.774685268329, test: 0.768211407364
step: 5000
NDCG@100 | train: 0.722038310887, test: 0.772169541271
step: 5100
NDCG@100 | train: 0.824666342745, test: 0.805788697004
step: 5200
NDCG@100 | train: 0.803495891161, test: 0.835907965152
step: 5300
NDCG@100 | train: 0.863959126376, test: 0.850426734018
step: 5400
NDCG@100 | train: 0.732305475977, test: 0.837972789522
step: 5500
NDCG@100 | train: 0.845631871154, test: 0.867182806036
step: 5600
NDCG@100 | train: 0.798421921736, test: 0.725478025922
step: 5700
NDCG@100 | train: 0.813857816488, test: 0.791579376116
step: 5800
NDCG@100 | train: 0.804200214306, test: 0.798769952427
step: 5900
NDCG@100 | train: 0.849634684887, test: 0.889521874225
step: 6000
NDCG@100 | train: 0.858612872927, test: 0.831914710089
step: 6100
NDCG@100 | train: 0.845986947553, test: 0.822197423278
step: 6200
NDCG@100 | train: 0.585902259974, test: 0.730900859859
step: 6300
NDCG@100 | train: 0.833367414575, test: 0.937506421609
step: 6400
NDCG@100 | train: 0.843419803569, test: 0.913120263852
step: 6500
NDCG@100 | train: 0.906232338369, test: 0.913914848465
step: 6600
NDCG@100 | train: 0.792874685923, test: 0.655276151041
step: 6700
NDCG@100 | train: 0.570053819726, test: 0.518515925526
step: 6800
NDCG@100 | train: 0.795004517952, test: 0.655787583569
step: 6900
NDCG@100 | train: 0.863977982718, test: 0.782881550183
step: 7000
NDCG@100 | train: 0.881024778281, test: 0.746460029399
step: 7100
NDCG@100 | train: 0.893541981228, test: 0.735299969439
step: 7200
NDCG@100 | train: 0.917921622496, test: 0.905750861115
step: 7300
NDCG@100 | train: 0.844878843501, test: 0.770582034174
step: 7400
NDCG@100 | train: 0.899487541833, test: 0.90547456491
step: 7500
NDCG@100 | train: 0.842162464796, test: 0.876203649972
step: 7600
NDCG@100 | train: 0.815080199287, test: 0.834392900376
step: 7700
NDCG@100 | train: 0.834017019886, test: 0.691710498597
step: 7800
NDCG@100 | train: 0.755980353443, test: 0.626546542858
step: 7900
NDCG@100 | train: 0.686251303512, test: 0.746332557737
step: 8000
NDCG@100 | train: 0.854987812765, test: 0.879810462577
step: 8100
NDCG@100 | train: 0.879424144715, test: 0.856877572913
step: 8200
NDCG@100 | train: 0.894952706369, test: 0.892881141007
step: 8300
NDCG@100 | train: 0.957377431421, test: 0.919214241848
step: 8400
NDCG@100 | train: 0.94344642731, test: 0.939447859025
step: 8500
NDCG@100 | train: 0.9255602476, test: 0.930483143809
step: 8600
NDCG@100 | train: 0.90158812629, test: 0.945843153488
step: 8700
NDCG@100 | train: 0.88304674637, test: 0.952804519272
step: 8800
NDCG@100 | train: 0.830361720706, test: 0.917866343638
step: 8900
NDCG@100 | train: 0.929126939884, test: 0.828054812871
step: 9000
NDCG@100 | train: 0.895246460932, test: 0.722637821856
step: 9100
NDCG@100 | train: 0.933638205315, test: 0.861213873163
step: 9200
NDCG@100 | train: 0.958885002191, test: 0.844911032823
step: 9300
NDCG@100 | train: 0.936811482985, test: 0.816425806863
step: 9400
NDCG@100 | train: 0.96099192111, test: 0.95086302326
step: 9500
NDCG@100 | train: 0.910082209115, test: 0.970157538568
step: 9600
NDCG@100 | train: 0.875985079256, test: 0.797831918963
step: 9700
NDCG@100 | train: 0.912014102569, test: 0.906355764094
step: 9800
NDCG@100 | train: 0.906594632995, test: 0.849937009726
step: 9900
NDCG@100 | train: 0.938182979607, test: 0.865497516888
step: 10000
NDCG@100 | train: 0.852505050694, test: 0.783859464691
save the model
save the optimizer
5
5
5
5
5
5
5
5
5
5
3
3
3
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2
4
4
4
4
4
4
4
4
4
8
8
8
8
8
8
8
8
8
8
8
8
6
6
6
6
6
6
6
6
6
6
6
6
1
1
1
1
1
1
1
1
1
1
1
1
1
7
7
7
7
7
7
7
7
7
7
7
7
7
(680, 680)
load dataset
('The number of data, train:', 646, 'validate:', 34)
prepare initialized model!
step: 100
NDCG@100 | train: 0.537292910788, test: 0.807732534974
step: 200
NDCG@100 | train: 0.595197627863, test: 0.926654461474
step: 300
NDCG@100 | train: 0.242964745557, test: 0.543416323241
step: 400
NDCG@100 | train: 0.580636777101, test: 0.844181972033
step: 500
NDCG@100 | train: 0.74750769615, test: 0.647371504049
step: 600
NDCG@100 | train: 0.495345498649, test: 0.728405532982
step: 700
NDCG@100 | train: 0.299912682008, test: 0.835580765456
step: 800
NDCG@100 | train: 0.787681984802, test: 0.846544687601
step: 900
NDCG@100 | train: 0.788233546824, test: 0.884296756536
step: 1000
NDCG@100 | train: 0.760389505936, test: 0.858511575212
step: 1100
NDCG@100 | train: 0.335838050145, test: 0.73999506149
step: 1200
NDCG@100 | train: 0.41307877395, test: 0.835580765456
step: 1300
NDCG@100 | train: 0.325433712416, test: 0.73999506149
step: 1400
NDCG@100 | train: 0.850496774167, test: 0.903506349833
step: 1500
NDCG@100 | train: 0.806908540167, test: 0.828401923367
step: 1600
NDCG@100 | train: 0.803571477806, test: 0.948860335433
step: 1700
NDCG@100 | train: 0.790900508151, test: 0.922394656962
step: 1800
NDCG@100 | train: 0.838291528292, test: 0.902508934134
step: 1900
NDCG@100 | train: 0.787826186779, test: 0.913703378718
step: 2000
NDCG@100 | train: 0.545630923113, test: 0.835580765456
step: 2100
NDCG@100 | train: 0.852929281827, test: 0.948041210042
step: 2200
NDCG@100 | train: 0.437442997299, test: 0.78393123829
step: 2300
NDCG@100 | train: 0.809311524116, test: 0.950842687022
step: 2400
NDCG@100 | train: 0.900257368874, test: 0.895904254691
step: 2500
NDCG@100 | train: 0.908814574521, test: 0.912145557006
step: 2600
NDCG@100 | train: 0.816202155708, test: 0.891596777012
step: 2700
NDCG@100 | train: 0.837853343918, test: 0.815803046154
step: 2800
NDCG@100 | train: 0.737673529264, test: 0.880104924765
step: 2900
NDCG@100 | train: 0.866389009451, test: 0.849294147949
step: 3000
NDCG@100 | train: 0.854207088189, test: 0.918595036336
step: 3100
NDCG@100 | train: 0.83099399783, test: 0.916627840332
step: 3200
NDCG@100 | train: 0.839572259645, test: 0.953788985438
step: 3300
NDCG@100 | train: 0.869517458441, test: 0.959488780207
step: 3400
NDCG@100 | train: 0.891677546065, test: 0.950980243717
step: 3500
NDCG@100 | train: 0.908531607446, test: 0.914637351408
step: 3600
NDCG@100 | train: 0.762538067541, test: 0.820776812526
step: 3700
NDCG@100 | train: 0.891293736535, test: 0.926303389421
step: 3800
NDCG@100 | train: 0.878223016887, test: 0.922635081642
step: 3900
NDCG@100 | train: 0.861588037412, test: 0.908592992636
step: 4000
NDCG@100 | train: 0.927842027659, test: 0.928854534687
step: 4100
NDCG@100 | train: 0.860941186463, test: 0.919436705125
step: 4200
NDCG@100 | train: 0.846642022948, test: 0.847510225303
step: 4300
NDCG@100 | train: 0.897155582976, test: 0.907885779128
step: 4400
NDCG@100 | train: 0.905189561891, test: 0.906023037978
step: 4500
NDCG@100 | train: 0.930401979056, test: 0.894370232228
step: 4600
NDCG@100 | train: 0.868171695376, test: 0.91385937675
step: 4700
NDCG@100 | train: 0.921270183488, test: 0.946895058386
step: 4800
NDCG@100 | train: 0.850900403362, test: 0.977539218773
step: 4900
NDCG@100 | train: 0.938911620456, test: 0.973001049132
step: 5000
NDCG@100 | train: 0.896900363945, test: 0.958740025668
step: 5100
NDCG@100 | train: 0.861738430727, test: 0.928760993692
step: 5200
NDCG@100 | train: 0.898390901045, test: 0.961754914117
step: 5300
NDCG@100 | train: 0.893952627342, test: 0.954187569447
step: 5400
NDCG@100 | train: 0.910716144661, test: 0.928269984226
step: 5500
NDCG@100 | train: 0.833319367414, test: 0.953195727698
step: 5600
NDCG@100 | train: 0.871993905724, test: 0.935768936351
step: 5700
NDCG@100 | train: 0.881450467035, test: 0.862731235908
step: 5800
NDCG@100 | train: 0.916478778508, test: 0.964914443606
step: 5900
NDCG@100 | train: 0.914759964491, test: 0.974855927134
step: 6000
NDCG@100 | train: 0.863744525647, test: 0.971007394056
step: 6100
NDCG@100 | train: 0.730694640272, test: 0.73999506149
step: 6200
NDCG@100 | train: 0.861374651772, test: 0.950982509722
step: 6300
NDCG@100 | train: 0.958357662332, test: 0.966930774316
step: 6400
NDCG@100 | train: 0.922802445148, test: 0.951135093563
step: 6500
NDCG@100 | train: 0.95271627368, test: 0.973893932927
step: 6600
NDCG@100 | train: 0.908789905224, test: 0.977322382078
step: 6700
NDCG@100 | train: 0.932719813551, test: 0.973287900158
step: 6800
NDCG@100 | train: 0.931832639011, test: 0.967962814546
step: 6900
NDCG@100 | train: 0.925379531459, test: 0.974202564793
step: 7000
NDCG@100 | train: 0.87918593442, test: 0.963328783321
step: 7100
NDCG@100 | train: 0.938512757011, test: 0.969833381519
step: 7200
NDCG@100 | train: 0.904747000218, test: 0.963795128826
step: 7300
NDCG@100 | train: 0.937633445907, test: 0.975970207098
step: 7400
NDCG@100 | train: 0.936417273107, test: 0.966856923407
step: 7500
NDCG@100 | train: 0.897860128551, test: 0.97848943752
step: 7600
NDCG@100 | train: 0.946006363804, test: 0.96792613983
step: 7700
NDCG@100 | train: 0.928697881599, test: 0.946573243489
step: 7800
NDCG@100 | train: 0.90853978371, test: 0.894396028735
step: 7900
NDCG@100 | train: 0.945616727887, test: 0.975255690732
step: 8000
NDCG@100 | train: 0.926702995774, test: 0.938994283892
step: 8100
NDCG@100 | train: 0.929942184645, test: 0.967931149164
step: 8200
NDCG@100 | train: 0.930224028057, test: 0.965987750786
step: 8300
NDCG@100 | train: 0.918597884455, test: 0.897372011434
step: 8400
NDCG@100 | train: 0.972590211979, test: 0.910093655008
step: 8500
NDCG@100 | train: 0.934369001752, test: 0.913318531827
step: 8600
NDCG@100 | train: 0.893979644644, test: 0.952718938053
step: 8700
NDCG@100 | train: 0.935287717732, test: 0.953738633334
step: 8800
NDCG@100 | train: 0.795524538962, test: 0.884780937505
step: 8900
NDCG@100 | train: 0.948858968141, test: 0.957878070103
step: 9000
NDCG@100 | train: 0.917316524723, test: 0.943148248067
step: 9100
NDCG@100 | train: 0.947865431488, test: 0.880896760795
step: 9200
NDCG@100 | train: 0.948344821738, test: 0.922611484683
step: 9300
NDCG@100 | train: 0.966858121639, test: 0.921202490849
step: 9400
NDCG@100 | train: 0.952309097295, test: 0.927686468757
step: 9500
NDCG@100 | train: 0.926154676327, test: 0.939516521805
step: 9600
NDCG@100 | train: 0.920725029799, test: 0.979725237573
step: 9700
NDCG@100 | train: 0.930488207043, test: 0.967433308185
step: 9800
NDCG@100 | train: 0.920627943847, test: 0.978685721223
step: 9900
NDCG@100 | train: 0.933722584295, test: 0.962658665517
step: 10000
NDCG@100 | train: 0.964304709706, test: 0.967469334171
save the model
save the optimizer
8
8
8
8
8
8
8
8
8
8
4
4
4
4
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2
2
6
6
6
6
6
6
6
6
6
6
6
6
7
7
7
7
7
7
7
7
7
7
7
7
1
1
1
1
1
1
1
1
1
1
1
1
1
5
5
5
5
5
5
5
5
5
5
5
5
5
(680, 680)
load dataset
('The number of data, train:', 646, 'validate:', 34)
prepare initialized model!
step: 100
NDCG@100 | train: 0.512417040236, test: 0.782606838724
step: 200
NDCG@100 | train: 0.308303963811, test: 0.523264601217
step: 300
NDCG@100 | train: 0.345899987855, test: 0.62991060012
step: 400
NDCG@100 | train: 0.787428585659, test: 0.8447908147
step: 500
NDCG@100 | train: 0.478793035327, test: 0.621846559223
step: 600
NDCG@100 | train: 0.645124041366, test: 0.856812862915
step: 700
NDCG@100 | train: 0.638062892857, test: 0.813504368694
step: 800
NDCG@100 | train: 0.346276417256, test: 0.523264601217
step: 900
NDCG@100 | train: 0.868606278187, test: 0.858375638035
step: 1000
NDCG@100 | train: 0.597068195559, test: 0.658799995666
step: 1100
NDCG@100 | train: 0.308303963811, test: 0.523264601217
step: 1200
NDCG@100 | train: 0.560530343012, test: 0.680993550419
step: 1300
NDCG@100 | train: 0.694675498135, test: 0.758232567233
step: 1400
NDCG@100 | train: 0.308303963811, test: 0.523264601217
step: 1500
NDCG@100 | train: 0.731522574077, test: 0.759238370118
step: 1600
NDCG@100 | train: 0.730381956333, test: 0.790094462434
step: 1700
NDCG@100 | train: 0.804583298377, test: 0.823226055502
step: 1800
NDCG@100 | train: 0.669194001458, test: 0.710788986878
step: 1900
NDCG@100 | train: 0.756599971468, test: 0.806040697256
step: 2000
NDCG@100 | train: 0.637649890872, test: 0.741550607356
step: 2100
NDCG@100 | train: 0.831349778397, test: 0.820389604254
step: 2200
NDCG@100 | train: 0.70134738802, test: 0.769271276902
step: 2300
NDCG@100 | train: 0.778077666747, test: 0.812312133435
step: 2400
NDCG@100 | train: 0.810347756588, test: 0.75473532714
step: 2500
NDCG@100 | train: 0.884602887171, test: 0.825293170565
step: 2600
NDCG@100 | train: 0.852428694995, test: 0.928705719353
step: 2700
NDCG@100 | train: 0.817471992303, test: 0.788363703937
step: 2800
NDCG@100 | train: 0.772864895737, test: 0.893131526257
step: 2900
NDCG@100 | train: 0.682750252491, test: 0.786647044068
step: 3000
NDCG@100 | train: 0.816573122847, test: 0.954337483514
step: 3100
NDCG@100 | train: 0.811201454758, test: 0.753742385141
step: 3200
NDCG@100 | train: 0.786856794816, test: 0.927756498243
step: 3300
NDCG@100 | train: 0.805573854499, test: 0.922008589976
step: 3400
NDCG@100 | train: 0.804905020531, test: 0.931521970099
step: 3500
NDCG@100 | train: 0.888403754508, test: 0.914134244735
step: 3600
NDCG@100 | train: 0.854812519472, test: 0.930216814914
step: 3700
NDCG@100 | train: 0.860623809206, test: 0.935792572013
step: 3800
NDCG@100 | train: 0.829052920797, test: 0.922775572691
step: 3900
NDCG@100 | train: 0.779232746147, test: 0.909630363328
step: 4000
NDCG@100 | train: 0.84110657796, test: 0.947274123915
step: 4100
NDCG@100 | train: 0.776423487025, test: 0.91858864457
step: 4200
NDCG@100 | train: 0.869553621644, test: 0.907802015812
step: 4300
NDCG@100 | train: 0.83233841474, test: 0.924323595783
step: 4400
NDCG@100 | train: 0.8862138305, test: 0.928381323714
step: 4500
NDCG@100 | train: 0.850827970639, test: 0.808099520361
step: 4600
NDCG@100 | train: 0.767806930272, test: 0.771133485222
step: 4700
NDCG@100 | train: 0.864523618435, test: 0.796789347032
step: 4800
NDCG@100 | train: 0.926376260593, test: 0.918347414257
step: 4900
NDCG@100 | train: 0.879320469864, test: 0.820265718439
step: 5000
NDCG@100 | train: 0.900807777599, test: 0.916741802761
step: 5100
NDCG@100 | train: 0.889050924406, test: 0.927273231701
step: 5200
NDCG@100 | train: 0.936848113404, test: 0.909161824691
step: 5300
NDCG@100 | train: 0.906966095661, test: 0.953272655603
step: 5400
NDCG@100 | train: 0.888318651329, test: 0.90422452463
step: 5500
NDCG@100 | train: 0.908197961131, test: 0.928650135911
step: 5600
NDCG@100 | train: 0.792940061945, test: 0.792284543771
step: 5700
NDCG@100 | train: 0.660302126568, test: 0.770501652195
step: 5800
NDCG@100 | train: 0.773309983992, test: 0.92994356146
step: 5900
NDCG@100 | train: 0.907156234572, test: 0.934173830933
step: 6000
NDCG@100 | train: 0.785154389444, test: 0.778974820132
step: 6100
NDCG@100 | train: 0.967832223708, test: 0.946770782658
step: 6200
NDCG@100 | train: 0.958246842741, test: 0.952119822342
step: 6300
NDCG@100 | train: 0.919398850364, test: 0.848676032265
step: 6400
NDCG@100 | train: 0.77697009832, test: 0.772383164821
step: 6500
NDCG@100 | train: 0.926099343648, test: 0.949048923994
step: 6600
NDCG@100 | train: 0.875471961128, test: 0.878520735072
step: 6700
NDCG@100 | train: 0.912339126123, test: 0.908779643509
step: 6800
NDCG@100 | train: 0.890937183227, test: 0.914575338662
step: 6900
NDCG@100 | train: 0.943132129826, test: 0.919902946127
step: 7000
NDCG@100 | train: 0.851300899375, test: 0.829991582058
step: 7100
NDCG@100 | train: 0.888642765602, test: 0.928909729073
step: 7200
NDCG@100 | train: 0.940680868877, test: 0.92338454076
step: 7300
NDCG@100 | train: 0.903578562965, test: 0.923042546784
step: 7400
NDCG@100 | train: 0.822621795138, test: 0.832326210351
step: 7500
NDCG@100 | train: 0.829426171284, test: 0.828981435087
step: 7600
NDCG@100 | train: 0.886319980718, test: 0.932044338227
step: 7700
NDCG@100 | train: 0.845944340148, test: 0.787963178228
step: 7800
NDCG@100 | train: 0.882550986212, test: 0.94696443224
step: 7900
NDCG@100 | train: 0.850453151702, test: 0.932734199666
step: 8000
NDCG@100 | train: 0.902644007073, test: 0.934283494454
step: 8100
NDCG@100 | train: 0.865740524383, test: 0.833548098633
step: 8200
NDCG@100 | train: 0.900658480299, test: 0.80959708582
step: 8300
NDCG@100 | train: 0.901084308131, test: 0.933061478227
step: 8400
NDCG@100 | train: 0.894539280616, test: 0.941622511731
step: 8500
NDCG@100 | train: 0.901515550142, test: 0.914794913318
step: 8600
NDCG@100 | train: 0.919461286839, test: 0.944352036637
step: 8700
NDCG@100 | train: 0.907666511558, test: 0.920754228998
step: 8800
NDCG@100 | train: 0.911571869433, test: 0.938808286787
step: 8900
NDCG@100 | train: 0.883126017744, test: 0.793358194708
step: 9000
NDCG@100 | train: 0.871256309995, test: 0.76631818726
step: 9100
NDCG@100 | train: 0.891936375048, test: 0.88316947267
step: 9200
NDCG@100 | train: 0.930836138996, test: 0.934564631788
step: 9300
NDCG@100 | train: 0.901888369437, test: 0.934651054713
step: 9400
NDCG@100 | train: 0.921679674028, test: 0.918740949135
step: 9500
NDCG@100 | train: 0.974280482837, test: 0.944235798763
step: 9600
NDCG@100 | train: 0.963009511037, test: 0.947168805546
step: 9700
NDCG@100 | train: 0.909485650708, test: 0.845157492524
step: 9800
NDCG@100 | train: 0.93906453259, test: 0.876693291195
step: 9900
NDCG@100 | train: 0.927107760788, test: 0.950620397761
step: 10000
NDCG@100 | train: 0.944456635132, test: 0.894398986346
save the model
save the optimizer
5
5
5
5
5
5
5
5
5
5
6
6
6
6
6
6
6
6
6
6
6
7
7
7
7
7
7
7
7
7
7
7
7
1
1
1
1
1
1
1
1
1
3
3
3
3
3
3
3
3
3
3
3
3
4
4
4
4
4
4
4
4
4
4
4
4
8
8
8
8
8
8
8
8
8
8
8
8
8
2
2
2
2
2
2
2
2
2
2
2
2
2
(680, 680)
load dataset
('The number of data, train:', 646, 'validate:', 34)
prepare initialized model!
step: 100
NDCG@100 | train: 0.528460622561, test: 0.881222949623
step: 200
NDCG@100 | train: 0.555016399216, test: 0.904098187527
step: 300
NDCG@100 | train: 0.512332706233, test: 0.844868785574
step: 400
NDCG@100 | train: 0.333245790998, test: 0.637595394831
step: 500
NDCG@100 | train: 0.593337644475, test: 0.927206701566
step: 600
NDCG@100 | train: 0.481674400099, test: 0.726407818105
step: 700
NDCG@100 | train: 0.591641716971, test: 0.85815912887
step: 800
NDCG@100 | train: 0.623051482704, test: 0.777769653407
step: 900
NDCG@100 | train: 0.69101248077, test: 0.827522684266
step: 1000
NDCG@100 | train: 0.695036931252, test: 0.929642049081
step: 1100
NDCG@100 | train: 0.687167098278, test: 0.887781089234
step: 1200
NDCG@100 | train: 0.698441717876, test: 0.85815912887
step: 1300
NDCG@100 | train: 0.734262131782, test: 0.915741610718
step: 1400
NDCG@100 | train: 0.690897242832, test: 0.824341106407
step: 1500
NDCG@100 | train: 0.371060607477, test: 0.637595394831
step: 1600
NDCG@100 | train: 0.688693149912, test: 0.955118464541
step: 1700
NDCG@100 | train: 0.741815527961, test: 0.970321295923
step: 1800
NDCG@100 | train: 0.666310310976, test: 0.919580566034
step: 1900
NDCG@100 | train: 0.73590750299, test: 0.861744356667
step: 2000
NDCG@100 | train: 0.71947188848, test: 0.972853265731
step: 2100
NDCG@100 | train: 0.745150820722, test: 0.976070713867
step: 2200
NDCG@100 | train: 0.710458825196, test: 0.955885603483
step: 2300
NDCG@100 | train: 0.696744215775, test: 0.826309216859
step: 2400
NDCG@100 | train: 0.782684991903, test: 0.994680751742
step: 2500
NDCG@100 | train: 0.796738707695, test: 0.911981020753
step: 2600
NDCG@100 | train: 0.825442792354, test: 0.971465266668
step: 2700
NDCG@100 | train: 0.794284562113, test: 0.942160666024
step: 2800
NDCG@100 | train: 0.794363133363, test: 0.943961981937
step: 2900
NDCG@100 | train: 0.777555177724, test: 0.94981381349
step: 3000
NDCG@100 | train: 0.777713846468, test: 0.900294234772
step: 3100
NDCG@100 | train: 0.816336214277, test: 0.97226122736
step: 3200
NDCG@100 | train: 0.76536622919, test: 0.911981020753
step: 3300
NDCG@100 | train: 0.770260795202, test: 0.963007221359
step: 3400
NDCG@100 | train: 0.80899206409, test: 0.952404512969
step: 3500
NDCG@100 | train: 0.790100328982, test: 0.946984094643
step: 3600
NDCG@100 | train: 0.782580504252, test: 0.916644380198
step: 3700
NDCG@100 | train: 0.828575048366, test: 0.969606553117
step: 3800
NDCG@100 | train: 0.859035376082, test: 0.994103672389
step: 3900
NDCG@100 | train: 0.820703178191, test: 0.97800005822
step: 4000
NDCG@100 | train: 0.841782714631, test: 0.935718525651
step: 4100
NDCG@100 | train: 0.838376232933, test: 0.964923666063
step: 4200
NDCG@100 | train: 0.852204720852, test: 0.983910417075
step: 4300
NDCG@100 | train: 0.871224056193, test: 0.979966667723
step: 4400
NDCG@100 | train: 0.85200465332, test: 0.955249052382
step: 4500
NDCG@100 | train: 0.853132837876, test: 0.975176219143
step: 4600
NDCG@100 | train: 0.813021334461, test: 0.899323907906
step: 4700
NDCG@100 | train: 0.835791418478, test: 0.945515817693
step: 4800
NDCG@100 | train: 0.725746666856, test: 0.794289626362
step: 4900
NDCG@100 | train: 0.796673775753, test: 0.863335399855
step: 5000
NDCG@100 | train: 0.812180735651, test: 0.867809640619
step: 5100
NDCG@100 | train: 0.772592799325, test: 0.947448589994
step: 5200
NDCG@100 | train: 0.822209148903, test: 0.962335076445
step: 5300
NDCG@100 | train: 0.798463912901, test: 0.979636824496
step: 5400
NDCG@100 | train: 0.815722792457, test: 0.951095896222
step: 5500
NDCG@100 | train: 0.818789347222, test: 0.851398350421
step: 5600
NDCG@100 | train: 0.845147399476, test: 0.977757674883
step: 5700
NDCG@100 | train: 0.831865956938, test: 0.939716469743
step: 5800
NDCG@100 | train: 0.850153507692, test: 0.955054451894
step: 5900
NDCG@100 | train: 0.862387780362, test: 0.956166714589
step: 6000
NDCG@100 | train: 0.833170151406, test: 0.954942582134
step: 6100
NDCG@100 | train: 0.86591324089, test: 0.974918311124
step: 6200
NDCG@100 | train: 0.844505377586, test: 0.957282059321
step: 6300
NDCG@100 | train: 0.842525479602, test: 0.953036356333
step: 6400
NDCG@100 | train: 0.860258728919, test: 0.957477754451
step: 6500
NDCG@100 | train: 0.814669055557, test: 0.910596178861
step: 6600
NDCG@100 | train: 0.833660423527, test: 0.960491618581
step: 6700
NDCG@100 | train: 0.875061752138, test: 0.96496388572
step: 6800
NDCG@100 | train: 0.863740106581, test: 0.904577982016
step: 6900
NDCG@100 | train: 0.893653152936, test: 0.962225123701
step: 7000
NDCG@100 | train: 0.88682156394, test: 0.973599681451
step: 7100
NDCG@100 | train: 0.847724495343, test: 0.876767963668
step: 7200
NDCG@100 | train: 0.891951723577, test: 0.950480649037
step: 7300
NDCG@100 | train: 0.858772029373, test: 0.952885930244
step: 7400
NDCG@100 | train: 0.911188880754, test: 0.969814196451
step: 7500
NDCG@100 | train: 0.90345253955, test: 0.972024317109
step: 7600
NDCG@100 | train: 0.855091690025, test: 0.88603303318
step: 7700
NDCG@100 | train: 0.859973477014, test: 0.905855433977
step: 7800
NDCG@100 | train: 0.886388159834, test: 0.963119747991
step: 7900
NDCG@100 | train: 0.832740715775, test: 0.965151104493
step: 8000
NDCG@100 | train: 0.264632682203, test: 0.637595394831
step: 8100
NDCG@100 | train: 0.904183855007, test: 0.989124190857
step: 8200
NDCG@100 | train: 0.87815650769, test: 0.976981111709
step: 8300
NDCG@100 | train: 0.910167677276, test: 0.976425220855
step: 8400
NDCG@100 | train: 0.912850869995, test: 0.972902630663
step: 8500
NDCG@100 | train: 0.924536246472, test: 0.979291706525
step: 8600
NDCG@100 | train: 0.870087615392, test: 0.939485780492
step: 8700
NDCG@100 | train: 0.902816522021, test: 0.973611818413
step: 8800
NDCG@100 | train: 0.916976358416, test: 0.955064716949
step: 8900
NDCG@100 | train: 0.87939215989, test: 0.975512525462
step: 9000
NDCG@100 | train: 0.912653464039, test: 0.966858726932
step: 9100
NDCG@100 | train: 0.923666724775, test: 0.972800669938
step: 9200
NDCG@100 | train: 0.90324872417, test: 0.95799728112
step: 9300
NDCG@100 | train: 0.91247748757, test: 0.986137684624
step: 9400
NDCG@100 | train: 0.904436412597, test: 0.976243008274
step: 9500
NDCG@100 | train: 0.889042092836, test: 0.969593296697
step: 9600
NDCG@100 | train: 0.725118323356, test: 0.783318247631
step: 9700
NDCG@100 | train: 0.936249600038, test: 0.985404695447
step: 9800
NDCG@100 | train: 0.961101995221, test: 0.98484676201
step: 9900
NDCG@100 | train: 0.958937787752, test: 0.982887746732
step: 10000
NDCG@100 | train: 0.951957622973, test: 0.986251697432
save the model
save the optimizer
6
6
6
6
6
6
6
6
6
6
7
7
7
7
7
7
7
7
7
7
7
5
5
5
5
5
5
5
5
5
5
5
5
8
8
8
8
8
8
8
8
8
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
4
4
4
4
4
4
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
3
3
3
3
3
(680, 680)
load dataset
('The number of data, train:', 646, 'validate:', 34)
prepare initialized model!
step: 100
NDCG@100 | train: 0.445835755732, test: 0.584799175877
step: 200
NDCG@100 | train: 0.240516477102, test: 0.672103228635
step: 300
NDCG@100 | train: 0.485172437121, test: 0.617443167679
step: 400
NDCG@100 | train: 0.592615659525, test: 0.587345765604
step: 500
NDCG@100 | train: 0.269923741812, test: 0.672103228635
step: 600
NDCG@100 | train: 0.293612952104, test: 0.672103228635
step: 700
NDCG@100 | train: 0.481567683277, test: 0.630921858767
step: 800
NDCG@100 | train: 0.807539496175, test: 0.768320370789
step: 900
NDCG@100 | train: 0.57802576358, test: 0.675397227742
step: 1000
NDCG@100 | train: 0.721872514742, test: 0.836464130962
step: 1100
NDCG@100 | train: 0.626147737463, test: 0.755624175634
step: 1200
NDCG@100 | train: 0.70707440237, test: 0.766292986757
step: 1300
NDCG@100 | train: 0.756052360501, test: 0.727896209369
step: 1400
NDCG@100 | train: 0.718471045351, test: 0.725393426197
step: 1500
NDCG@100 | train: 0.791762269226, test: 0.860341857532
step: 1600
NDCG@100 | train: 0.667403107402, test: 0.713545285079
step: 1700
NDCG@100 | train: 0.608881193408, test: 0.672103228635
step: 1800
NDCG@100 | train: 0.745802246532, test: 0.835841940516
step: 1900
NDCG@100 | train: 0.833053902128, test: 0.867462080614
step: 2000
NDCG@100 | train: 0.67418318193, test: 0.660470942801
step: 2100
NDCG@100 | train: 0.62688812921, test: 0.655464066695
step: 2200
NDCG@100 | train: 0.742532636818, test: 0.668293141649
step: 2300
NDCG@100 | train: 0.802041904577, test: 0.735026404673
step: 2400
NDCG@100 | train: 0.679786409993, test: 0.663482066089
step: 2500
NDCG@100 | train: 0.762240281331, test: 0.706176160408
step: 2600
NDCG@100 | train: 0.798993926239, test: 0.695474848826
step: 2700
NDCG@100 | train: 0.773621369734, test: 0.856585877673
step: 2800
NDCG@100 | train: 0.78454876283, test: 0.748402009362
step: 2900
NDCG@100 | train: 0.642607999104, test: 0.76372824804
step: 3000
NDCG@100 | train: 0.785479186884, test: 0.665639424727
step: 3100
NDCG@100 | train: 0.790434059512, test: 0.885909547421
step: 3200
NDCG@100 | train: 0.732839223803, test: 0.787206088329
step: 3300
NDCG@100 | train: 0.797569499989, test: 0.723179363823
step: 3400
NDCG@100 | train: 0.841107884108, test: 0.8275320362
step: 3500
NDCG@100 | train: 0.903881443492, test: 0.904933547325
step: 3600
NDCG@100 | train: 0.885833449521, test: 0.88187049107
step: 3700
NDCG@100 | train: 0.820835449983, test: 0.686858711118
step: 3800
NDCG@100 | train: 0.839926069371, test: 0.764053465805
step: 3900
NDCG@100 | train: 0.936099638481, test: 0.797763858788
step: 4000
NDCG@100 | train: 0.685955201413, test: 0.668894796976
step: 4100
NDCG@100 | train: 0.81236809223, test: 0.749967045844
step: 4200
NDCG@100 | train: 0.504179327553, test: 0.672103228635
step: 4300
NDCG@100 | train: 0.883088698966, test: 0.942822831384
step: 4400
NDCG@100 | train: 0.801336461526, test: 0.755902185351
step: 4500
NDCG@100 | train: 0.889380824898, test: 0.854159441714
step: 4600
NDCG@100 | train: 0.865842564753, test: 0.860731850596
step: 4700
NDCG@100 | train: 0.893610094697, test: 0.94697821212
step: 4800
NDCG@100 | train: 0.854188108627, test: 0.893241086775
step: 4900
NDCG@100 | train: 0.831027121898, test: 0.890720256253
step: 5000
NDCG@100 | train: 0.899076178968, test: 0.894368745643
step: 5100
NDCG@100 | train: 0.920147966736, test: 0.902697651448
step: 5200
NDCG@100 | train: 0.889551118803, test: 0.87954117086
step: 5300
NDCG@100 | train: 0.775433966746, test: 0.899177319344
step: 5400
NDCG@100 | train: 0.870294411178, test: 0.799089377669
step: 5500
NDCG@100 | train: 0.864867230172, test: 0.78669094794
step: 5600
NDCG@100 | train: 0.938098799792, test: 0.770802126933
step: 5700
NDCG@100 | train: 0.936392642006, test: 0.759321743779
step: 5800
NDCG@100 | train: 0.923108950125, test: 0.748051197477
step: 5900
NDCG@100 | train: 0.9378150993, test: 0.788921799112
step: 6000
NDCG@100 | train: 0.971445268245, test: 0.811431099466
step: 6100
NDCG@100 | train: 0.963010621389, test: 0.807283723963
step: 6200
NDCG@100 | train: 0.919169559816, test: 0.727194736423
step: 6300
NDCG@100 | train: 0.960640282891, test: 0.788706825646
step: 6400
NDCG@100 | train: 0.966981402864, test: 0.785705192521
step: 6500
NDCG@100 | train: 0.977300903424, test: 0.948258189831
step: 6600
NDCG@100 | train: 0.9556788668, test: 0.787370139531
step: 6700
NDCG@100 | train: 0.896099479574, test: 0.784131949658
step: 6800
NDCG@100 | train: 0.962752384735, test: 0.757252381978
step: 6900
NDCG@100 | train: 0.967276470792, test: 0.889353424289
step: 7000
NDCG@100 | train: 0.932838664686, test: 0.86510935084
step: 7100
NDCG@100 | train: 0.961776677893, test: 0.925179992265
step: 7200
NDCG@100 | train: 0.963628791035, test: 0.855647757761
step: 7300
NDCG@100 | train: 0.917651887178, test: 0.756828398743
step: 7400
NDCG@100 | train: 0.91145267026, test: 0.725745119236
step: 7500
NDCG@100 | train: 0.977284222014, test: 0.935831736879
step: 7600
NDCG@100 | train: 0.893748671653, test: 0.679264174815
step: 7700
NDCG@100 | train: 0.953361123425, test: 0.771658459834
step: 7800
NDCG@100 | train: 0.95151919688, test: 0.827209749711
step: 7900
NDCG@100 | train: 0.976730484593, test: 0.958521345192
step: 8000
NDCG@100 | train: 0.947891823169, test: 0.810243367104
step: 8100
NDCG@100 | train: 0.939828921257, test: 0.781285648163
step: 8200
NDCG@100 | train: 0.970544441713, test: 0.802728338716
step: 8300
NDCG@100 | train: 0.915052844688, test: 0.80796760954
step: 8400
NDCG@100 | train: 0.948051868995, test: 0.859637656364
step: 8500
NDCG@100 | train: 0.92685640792, test: 0.881101013423
step: 8600
NDCG@100 | train: 0.963869419713, test: 0.953154462806
step: 8700
NDCG@100 | train: 0.907815527515, test: 0.809720387552
step: 8800
NDCG@100 | train: 0.812419423783, test: 0.756616091402
step: 8900
NDCG@100 | train: 0.95978299226, test: 0.834869164923
step: 9000
NDCG@100 | train: 0.940290010401, test: 0.913290080036
step: 9100
NDCG@100 | train: 0.949714367225, test: 0.946638334656
step: 9200
NDCG@100 | train: 0.972153980319, test: 0.889432048217
step: 9300
NDCG@100 | train: 0.929846755949, test: 0.843429544318
step: 9400
NDCG@100 | train: 0.968224166173, test: 0.926291294588
step: 9500
NDCG@100 | train: 0.894571221063, test: 0.884010648244
step: 9600
NDCG@100 | train: 0.95266301722, test: 0.863199417705
step: 9700
NDCG@100 | train: 0.915579878647, test: 0.858693412469
step: 9800
NDCG@100 | train: 0.97978597858, test: 0.918849380285
step: 9900
NDCG@100 | train: 0.959563621231, test: 0.825183757019
step: 10000
NDCG@100 | train: 0.976679642372, test: 0.808446956081
save the model
save the optimizer
4
4
4
4
4
4
4
4
4
4
6
6
6
6
6
6
6
6
6
6
6
5
5
5
5
5
5
5
5
5
5
5
5
2
2
2
2
2
2
2
2
2
1
1
1
1
1
1
1
1
1
1
1
1
3
3
3
3
3
3
3
3
3
3
3
3
7
7
7
7
7
7
7
7
7
7
7
7
7
8
8
8
8
8
8
8
8
8
8
8
8
8
(680, 680)
load dataset
('The number of data, train:', 646, 'validate:', 34)
prepare initialized model!
step: 100
NDCG@100 | train: 0.310484436555, test: 0.660442027749
step: 200
NDCG@100 | train: 0.190140713943, test: 0.551012206154
step: 300
NDCG@100 | train: 0.344201814017, test: 0.661911636942
step: 400
NDCG@100 | train: 0.689844148968, test: 0.773597692715
step: 500
NDCG@100 | train: 0.802909274671, test: 0.892809660588
step: 600
NDCG@100 | train: 0.748729626853, test: 0.886722675408
step: 700
NDCG@100 | train: 0.835954845792, test: 0.90925610145
step: 800
NDCG@100 | train: 0.74010862309, test: 0.91951294948
step: 900
NDCG@100 | train: 0.816546985266, test: 0.883441555427
step: 1000
NDCG@100 | train: 0.513897402222, test: 0.880967184203
step: 1100
NDCG@100 | train: 0.362838760857, test: 0.551012206154
step: 1200
NDCG@100 | train: 0.534759078942, test: 0.772904296952
step: 1300
NDCG@100 | train: 0.473860190421, test: 0.744167346439
step: 1400
NDCG@100 | train: 0.667475794026, test: 0.843480894917
step: 1500
NDCG@100 | train: 0.205372697108, test: 0.551012206154
step: 1600
NDCG@100 | train: 0.853706654069, test: 0.923778863804
step: 1700
NDCG@100 | train: 0.834372034031, test: 0.908920276367
step: 1800
NDCG@100 | train: 0.81509208433, test: 0.884553955536
step: 1900
NDCG@100 | train: 0.731051170278, test: 0.853987440769
step: 2000
NDCG@100 | train: 0.436148151153, test: 0.744167346439
step: 2100
NDCG@100 | train: 0.641383392439, test: 0.853759974445
step: 2200
NDCG@100 | train: 0.833707328229, test: 0.873386912186
step: 2300
NDCG@100 | train: 0.79016712846, test: 0.867078793253
step: 2400
NDCG@100 | train: 0.864768878779, test: 0.855429632546
step: 2500
NDCG@100 | train: 0.896535858508, test: 0.90925610145
step: 2600
NDCG@100 | train: 0.829513763943, test: 0.871477610839
step: 2700
NDCG@100 | train: 0.244305756936, test: 0.551012206154
step: 2800
NDCG@100 | train: 0.671160350235, test: 0.732225976398
step: 2900
NDCG@100 | train: 0.869547715132, test: 0.801349678926
step: 3000
NDCG@100 | train: 0.907383778654, test: 0.85517614783
step: 3100
NDCG@100 | train: 0.932084069066, test: 0.890249972331
step: 3200
NDCG@100 | train: 0.862472946578, test: 0.931955872266
step: 3300
NDCG@100 | train: 0.87150650109, test: 0.867728494641
step: 3400
NDCG@100 | train: 0.925957725867, test: 0.920078660535
step: 3500
NDCG@100 | train: 0.938614525136, test: 0.957095167613
step: 3600
NDCG@100 | train: 0.865393385929, test: 0.933770218632
step: 3700
NDCG@100 | train: 0.858927645177, test: 0.868875556429
step: 3800
NDCG@100 | train: 0.914554264497, test: 0.879792154087
step: 3900
NDCG@100 | train: 0.938273485596, test: 0.895326879971
step: 4000
NDCG@100 | train: 0.864724260568, test: 0.887458320392
step: 4100
NDCG@100 | train: 0.887402560259, test: 0.931908680295
step: 4200
NDCG@100 | train: 0.88505999826, test: 0.946339436961
step: 4300
NDCG@100 | train: 0.852270074654, test: 0.808986743114
step: 4400
NDCG@100 | train: 0.818239971622, test: 0.857559251073
step: 4500
NDCG@100 | train: 0.866734911689, test: 0.911861818173
step: 4600
NDCG@100 | train: 0.848763730655, test: 0.878334438666
step: 4700
NDCG@100 | train: 0.88548001269, test: 0.947681004015
step: 4800
NDCG@100 | train: 0.862464338456, test: 0.927348713686
step: 4900
NDCG@100 | train: 0.897786889675, test: 0.948879097319
step: 5000
NDCG@100 | train: 0.889152986758, test: 0.953109291876
step: 5100
NDCG@100 | train: 0.788880186422, test: 0.842472601996
step: 5200
NDCG@100 | train: 0.883985827485, test: 0.941770627682
step: 5300
NDCG@100 | train: 0.910974341209, test: 0.960012443851
step: 5400
NDCG@100 | train: 0.8972648927, test: 0.953875285483
step: 5500
NDCG@100 | train: 0.901772442227, test: 0.946994281597
step: 5600
NDCG@100 | train: 0.900635700081, test: 0.951371384299
step: 5700
NDCG@100 | train: 0.91835032324, test: 0.918701897997
step: 5800
NDCG@100 | train: 0.948915782317, test: 0.961594119287
step: 5900
NDCG@100 | train: 0.980637085932, test: 0.961364002219
step: 6000
NDCG@100 | train: 0.963272142862, test: 0.937751579789
step: 6100
NDCG@100 | train: 0.974091307275, test: 0.955091642242
step: 6200
NDCG@100 | train: 0.973298782331, test: 0.964731680208
step: 6300
NDCG@100 | train: 0.95985570708, test: 0.901233473583
step: 6400
NDCG@100 | train: 0.956511288384, test: 0.906108538046
step: 6500
NDCG@100 | train: 0.93463413632, test: 0.874528090915
step: 6600
NDCG@100 | train: 0.933262668031, test: 0.909860150098
step: 6700
NDCG@100 | train: 0.957339270065, test: 0.881576335037
step: 6800
NDCG@100 | train: 0.839679921562, test: 0.765679773548
step: 6900
NDCG@100 | train: 0.890737565201, test: 0.85547688598
step: 7000
NDCG@100 | train: 0.244305756936, test: 0.551012206154
step: 7100
NDCG@100 | train: 0.978226061111, test: 0.958137617119
step: 7200
NDCG@100 | train: 0.900043478641, test: 0.757702614901
step: 7300
NDCG@100 | train: 0.969588411581, test: 0.902755202672
step: 7400
NDCG@100 | train: 0.963148944108, test: 0.952955387497
step: 7500
NDCG@100 | train: 0.970920480629, test: 0.936697209033
step: 7600
NDCG@100 | train: 0.975159325922, test: 0.941538274012
step: 7700
NDCG@100 | train: 0.944266111509, test: 0.958185560777
step: 7800
NDCG@100 | train: 0.975609562708, test: 0.965033881242
step: 7900
NDCG@100 | train: 0.965711553717, test: 0.843480894917
step: 8000
NDCG@100 | train: 0.753522665168, test: 0.820169317224
step: 8100
NDCG@100 | train: 0.979312153049, test: 0.961436476667
step: 8200
NDCG@100 | train: 0.965275151343, test: 0.939653031436
step: 8300
NDCG@100 | train: 0.961820328435, test: 0.950408937635
step: 8400
NDCG@100 | train: 0.972626567621, test: 0.961230957478
step: 8500
NDCG@100 | train: 0.983280812952, test: 0.974118027681
step: 8600
NDCG@100 | train: 0.980620280715, test: 0.957070478939
step: 8700
NDCG@100 | train: 0.984900938926, test: 0.953623405271
step: 8800
NDCG@100 | train: 0.957209554197, test: 0.924014814012
step: 8900
NDCG@100 | train: 0.900027253391, test: 0.911070433552
step: 9000
NDCG@100 | train: 0.980827617461, test: 0.953416991192
step: 9100
NDCG@100 | train: 0.917139667059, test: 0.862666063781
step: 9200
NDCG@100 | train: 0.947704176099, test: 0.942092378617
step: 9300
NDCG@100 | train: 0.97491028362, test: 0.956871913742
step: 9400
NDCG@100 | train: 0.956268589006, test: 0.949201043846
step: 9500
NDCG@100 | train: 0.949929620651, test: 0.943045467603
step: 9600
NDCG@100 | train: 0.985338067934, test: 0.978541920617
step: 9700
NDCG@100 | train: 0.987195936566, test: 0.980131247304
step: 9800
NDCG@100 | train: 0.964504327408, test: 0.924150264539
step: 9900
NDCG@100 | train: 0.918167263632, test: 0.947305579267
step: 10000
NDCG@100 | train: 0.954603297773, test: 0.958286293827
save the model
save the optimizer
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
8
8
8
8
8
8
8
8
8
8
8
8
7
7
7
7
7
7
7
7
7
4
4
4
4
4
4
4
4
4
4
4
4
6
6
6
6
6
6
6
6
6
6
6
6
5
5
5
5
5
5
5
5
5
5
5
5
5
3
3
3
3
3
3
3
3
3
3
3
3
3
(680, 680)
load dataset
('The number of data, train:', 646, 'validate:', 34)
prepare initialized model!
step: 100
NDCG@100 | train: 0.246826403956, test: 0.73419208986
step: 200
NDCG@100 | train: 0.334541087944, test: 0.749888829075
step: 300
NDCG@100 | train: 0.42340026411, test: 0.771439080856
step: 400
NDCG@100 | train: 0.22092811443, test: 0.820158113877
step: 500
NDCG@100 | train: 0.423030088478, test: 0.715102858344
step: 600
NDCG@100 | train: 0.767296875197, test: 0.828023432914
step: 700
NDCG@100 | train: 0.621945661437, test: 0.71319835722
step: 800
NDCG@100 | train: 0.657526514953, test: 0.792840570983
step: 900
NDCG@100 | train: 0.252790196406, test: 0.820158113877
step: 1000
NDCG@100 | train: 0.718127749708, test: 0.791943662186
step: 1100
NDCG@100 | train: 0.746353251001, test: 0.901458839151
step: 1200
NDCG@100 | train: 0.59343439426, test: 0.698592908008
step: 1300
NDCG@100 | train: 0.671915789443, test: 0.922822385718
step: 1400
NDCG@100 | train: 0.697295344691, test: 0.77281734218
step: 1500
NDCG@100 | train: 0.82728653174, test: 0.902546576512
step: 1600
NDCG@100 | train: 0.865718728426, test: 0.89932512247
step: 1700
NDCG@100 | train: 0.777224835663, test: 0.767536546893
step: 1800
NDCG@100 | train: 0.857498793972, test: 0.92055154288
step: 1900
NDCG@100 | train: 0.909219079049, test: 0.93726407153
step: 2000
NDCG@100 | train: 0.876959178885, test: 0.900985527971
step: 2100
NDCG@100 | train: 0.529020154313, test: 0.810514310776
step: 2200
NDCG@100 | train: 0.721527187851, test: 0.798548575539
step: 2300
NDCG@100 | train: 0.829778961946, test: 0.832287768753
step: 2400
NDCG@100 | train: 0.507513712975, test: 0.820158113877
step: 2500
NDCG@100 | train: 0.423785906256, test: 0.820158113877
step: 2600
NDCG@100 | train: 0.886656227049, test: 0.918600667124
step: 2700
NDCG@100 | train: 0.840646670306, test: 0.877586111984
step: 2800
NDCG@100 | train: 0.791484206132, test: 0.845479662681
step: 2900
NDCG@100 | train: 0.756067924298, test: 0.778963651987
step: 3000
NDCG@100 | train: 0.902672415139, test: 0.933140334962
step: 3100
NDCG@100 | train: 0.653149857378, test: 0.725076010552
step: 3200
NDCG@100 | train: 0.847526376212, test: 0.799399787429
step: 3300
NDCG@100 | train: 0.855264115932, test: 0.852785374436
step: 3400
NDCG@100 | train: 0.891663677543, test: 0.970151791247
step: 3500
NDCG@100 | train: 0.22092811443, test: 0.820158113877
step: 3600
NDCG@100 | train: 0.880612292054, test: 0.918660482494
step: 3700
NDCG@100 | train: 0.722175854873, test: 0.8918779591
step: 3800
NDCG@100 | train: 0.885424596249, test: 0.873999919512
step: 3900
NDCG@100 | train: 0.806971451549, test: 0.800484001381
step: 4000
NDCG@100 | train: 0.887799882894, test: 0.906367270314
step: 4100
NDCG@100 | train: 0.860606149599, test: 0.941855320756
step: 4200
NDCG@100 | train: 0.867233780208, test: 0.886068007979
step: 4300
NDCG@100 | train: 0.598519695676, test: 0.820158113877
step: 4400
NDCG@100 | train: 0.900081717207, test: 0.877280617987
step: 4500
NDCG@100 | train: 0.938319431579, test: 0.96247702808
step: 4600
NDCG@100 | train: 0.955333979786, test: 0.969794006694
step: 4700
NDCG@100 | train: 0.887811304521, test: 0.975986525412
step: 4800
NDCG@100 | train: 0.918398385979, test: 0.953711891349
step: 4900
NDCG@100 | train: 0.896680039551, test: 0.807213185117
step: 5000
NDCG@100 | train: 0.844372635382, test: 0.857009736879
step: 5100
NDCG@100 | train: 0.969723153592, test: 0.981596987535
step: 5200
NDCG@100 | train: 0.91327581937, test: 0.901081887627
step: 5300
NDCG@100 | train: 0.926067952028, test: 0.961117830459
step: 5400
NDCG@100 | train: 0.925439511494, test: 0.969316188774
step: 5500
NDCG@100 | train: 0.935348630265, test: 0.972237558142
step: 5600
NDCG@100 | train: 0.947508818333, test: 0.958692884534
step: 5700
NDCG@100 | train: 0.908677367941, test: 0.93748684783
step: 5800
NDCG@100 | train: 0.877416582571, test: 0.911125978501
step: 5900
NDCG@100 | train: 0.769685627419, test: 0.749126177159
step: 6000
NDCG@100 | train: 0.925095238553, test: 0.867866321182
step: 6100
NDCG@100 | train: 0.887050126253, test: 0.898035606918
step: 6200
NDCG@100 | train: 0.918051264051, test: 0.916146968583
step: 6300
NDCG@100 | train: 0.913700722365, test: 0.936650346577
step: 6400
NDCG@100 | train: 0.921978725516, test: 0.848251720649
step: 6500
NDCG@100 | train: 0.933050843485, test: 0.875405758559
step: 6600
NDCG@100 | train: 0.942371402447, test: 0.973455842444
step: 6700
NDCG@100 | train: 0.927557297409, test: 0.978114024024
step: 6800
NDCG@100 | train: 0.857902894587, test: 0.913270933548
step: 6900
NDCG@100 | train: 0.976849920472, test: 0.992522253979
step: 7000
NDCG@100 | train: 0.925649285631, test: 0.956942105882
step: 7100
NDCG@100 | train: 0.929338545195, test: 0.971528642077
step: 7200
NDCG@100 | train: 0.913751363106, test: 0.946049215493
step: 7300
NDCG@100 | train: 0.952604134135, test: 0.984551623629
step: 7400
NDCG@100 | train: 0.959548034845, test: 0.986721107545
step: 7500
NDCG@100 | train: 0.942176251068, test: 0.974852249715
step: 7600
NDCG@100 | train: 0.953620268704, test: 0.984592235535
step: 7700
NDCG@100 | train: 0.904217686008, test: 0.956969860469
step: 7800
NDCG@100 | train: 0.932937590935, test: 0.94907535668
step: 7900
NDCG@100 | train: 0.700004394218, test: 0.8608634402
step: 8000
NDCG@100 | train: 0.944700506032, test: 0.942991853453
step: 8100
NDCG@100 | train: 0.92858863897, test: 0.829962450307
step: 8200
NDCG@100 | train: 0.990219818288, test: 0.988291677978
step: 8300
NDCG@100 | train: 0.934451030009, test: 0.965551481468
step: 8400
NDCG@100 | train: 0.972455747162, test: 0.98921348295
step: 8500
NDCG@100 | train: 0.97331924204, test: 0.957674466677
step: 8600
NDCG@100 | train: 0.971240551803, test: 0.951063363608
step: 8700
NDCG@100 | train: 0.966224005279, test: 0.974573999186
step: 8800
NDCG@100 | train: 0.9874315686, test: 0.990659505181
step: 8900
NDCG@100 | train: 0.939251981057, test: 0.897894161521
step: 9000
NDCG@100 | train: 0.979118496295, test: 0.994419955703
step: 9100
NDCG@100 | train: 0.983437332131, test: 0.996081223164
step: 9200
NDCG@100 | train: 0.970512091887, test: 0.987753787636
step: 9300
NDCG@100 | train: 0.973373503039, test: 0.981428182472
step: 9400
NDCG@100 | train: 0.966631430126, test: 0.989585377614
step: 9500
NDCG@100 | train: 0.962627188981, test: 0.964810851906
step: 9600
NDCG@100 | train: 0.955309499076, test: 0.914715990723
step: 9700
NDCG@100 | train: 0.956527359957, test: 0.884517028238
step: 9800
NDCG@100 | train: 0.83310521796, test: 0.932042742216
step: 9900
NDCG@100 | train: 0.978780980076, test: 0.989683244284
step: 10000
NDCG@100 | train: 0.97079517434, test: 0.970300924706
save the model
save the optimizer
7
7
7
7
7
7
7
7
7
7
5
5
5
5
5
5
5
5
5
5
5
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
6
6
6
6
6
6
6
6
6
6
6
6
8
8
8
8
8
8
8
8
8
8
8
8
3
3
3
3
3
3
3
3
3
3
3
3
3
4
4
4
4
4
4
4
4
4
4
4
4
4
(680, 680)
load dataset
('The number of data, train:', 646, 'validate:', 34)
prepare initialized model!
step: 100
NDCG@100 | train: 0.736203572771, test: 0.956318930878
step: 200
NDCG@100 | train: 0.751131405811, test: 0.883765636029
step: 300
NDCG@100 | train: 0.776447379121, test: 0.964449491993
step: 400
NDCG@100 | train: 0.817540367038, test: 0.960023492077
step: 500
NDCG@100 | train: 0.301380878244, test: 0.599007162341
step: 600
NDCG@100 | train: 0.692768421123, test: 0.864644651179
step: 700
NDCG@100 | train: 0.833954669901, test: 0.968047030104
step: 800
NDCG@100 | train: 0.858392003151, test: 0.903152093484
step: 900
NDCG@100 | train: 0.78089231555, test: 0.865835374496
step: 1000
NDCG@100 | train: 0.279550148136, test: 0.599007162341
step: 1100
NDCG@100 | train: 0.831309833156, test: 0.887834671738
step: 1200
NDCG@100 | train: 0.853873969078, test: 0.899692358796
step: 1300
NDCG@100 | train: 0.824545720788, test: 0.974295705205
step: 1400
NDCG@100 | train: 0.887223638229, test: 0.98206294976
step: 1500
NDCG@100 | train: 0.804298317392, test: 0.910223485095
step: 1600
NDCG@100 | train: 0.605729776222, test: 0.84170761712
step: 1700
NDCG@100 | train: 0.893261477188, test: 0.97440272088
step: 1800
NDCG@100 | train: 0.909944689199, test: 0.993135912114
step: 1900
NDCG@100 | train: 0.903787888267, test: 0.993610105137
step: 2000
NDCG@100 | train: 0.925447652775, test: 0.969072047473
step: 2100
NDCG@100 | train: 0.880163238177, test: 0.969132322282
step: 2200
NDCG@100 | train: 0.885581143999, test: 0.977649996741
step: 2300
NDCG@100 | train: 0.869340215266, test: 0.987381728126
step: 2400
NDCG@100 | train: 0.899982450832, test: 0.991759508553
step: 2500
NDCG@100 | train: 0.868312067627, test: 0.980307166713
step: 2600
NDCG@100 | train: 0.899053969317, test: 0.918895477137
step: 2700
NDCG@100 | train: 0.931837760696, test: 0.979927359714
step: 2800
NDCG@100 | train: 0.942436736181, test: 0.987680967781
step: 2900
NDCG@100 | train: 0.871834780596, test: 0.968406751746
step: 3000
NDCG@100 | train: 0.861992159061, test: 0.917534598413
step: 3100
NDCG@100 | train: 0.849086578389, test: 0.893537076949
step: 3200
NDCG@100 | train: 0.907039268514, test: 0.889454268171
step: 3300
NDCG@100 | train: 0.906938431573, test: 0.983627626333
step: 3400
NDCG@100 | train: 0.801197786586, test: 0.859223871652
step: 3500
NDCG@100 | train: 0.870487303311, test: 0.869351861288
step: 3600
NDCG@100 | train: 0.92229766981, test: 0.913835248085
step: 3700
NDCG@100 | train: 0.895068136496, test: 0.920693981771
step: 3800
NDCG@100 | train: 0.775765915641, test: 0.865056334685
step: 3900
NDCG@100 | train: 0.827704484207, test: 0.996052523189
step: 4000
NDCG@100 | train: 0.829737561869, test: 0.863619434119
step: 4100
NDCG@100 | train: 0.932172565404, test: 0.982174167357
step: 4200
NDCG@100 | train: 0.857307640466, test: 0.982516955217
step: 4300
NDCG@100 | train: 0.885602514085, test: 0.877990839344
step: 4400
NDCG@100 | train: 0.880776242134, test: 0.95820612411
step: 4500
NDCG@100 | train: 0.819335606947, test: 0.902966067819
step: 4600
NDCG@100 | train: 0.858889550125, test: 0.99535970611
step: 4700
NDCG@100 | train: 0.920666290954, test: 0.991891757741
step: 4800
NDCG@100 | train: 0.876768108662, test: 0.863525833683
step: 4900
NDCG@100 | train: 0.802635450131, test: 0.863640957064
step: 5000
NDCG@100 | train: 0.873270418485, test: 0.869937566456
step: 5100
NDCG@100 | train: 0.885896573772, test: 0.878497437752
step: 5200
NDCG@100 | train: 0.914401055971, test: 0.95967857563
step: 5300
NDCG@100 | train: 0.906812815909, test: 0.894482595007
step: 5400
NDCG@100 | train: 0.908043314798, test: 0.914793462032
step: 5500
NDCG@100 | train: 0.927203132794, test: 0.916738772762
step: 5600
NDCG@100 | train: 0.935409398773, test: 0.984332381937
step: 5700
NDCG@100 | train: 0.929993739225, test: 0.903660392626
step: 5800
NDCG@100 | train: 0.881157171561, test: 0.868092949183
step: 5900
NDCG@100 | train: 0.733456176264, test: 0.856516121581
step: 6000
NDCG@100 | train: 0.802101379743, test: 0.863058264278
step: 6100
NDCG@100 | train: 0.92362867852, test: 0.980873409735
step: 6200
NDCG@100 | train: 0.868688066252, test: 0.868502410818
step: 6300
NDCG@100 | train: 0.935058054511, test: 0.99366106171
step: 6400
NDCG@100 | train: 0.902040830196, test: 0.94792234952
step: 6500
NDCG@100 | train: 0.912524485696, test: 0.906872607417
step: 6600
NDCG@100 | train: 0.925548317061, test: 0.97562374959
step: 6700
NDCG@100 | train: 0.921058866121, test: 0.964397656714
step: 6800
NDCG@100 | train: 0.887948455788, test: 0.863107033893
step: 6900
NDCG@100 | train: 0.914241144116, test: 0.964248226029
step: 7000
NDCG@100 | train: 0.88011839187, test: 0.861613631031
step: 7100
NDCG@100 | train: 0.886198078135, test: 0.866395039774
step: 7200
NDCG@100 | train: 0.934535213098, test: 0.986809324023
step: 7300
NDCG@100 | train: 0.934358496262, test: 0.880304280229
step: 7400
NDCG@100 | train: 0.939622587476, test: 0.904563816084
step: 7500
NDCG@100 | train: 0.953705617678, test: 0.997309310147
step: 7600
NDCG@100 | train: 0.942063907365, test: 0.99477642213
step: 7700
NDCG@100 | train: 0.925943758502, test: 0.878294366321
step: 7800
NDCG@100 | train: 0.907183123279, test: 0.888418061371
step: 7900
NDCG@100 | train: 0.911606365209, test: 0.877596051388
step: 8000
NDCG@100 | train: 0.859585238938, test: 0.867442977281
step: 8100
NDCG@100 | train: 0.936377474905, test: 0.872206526897
step: 8200
NDCG@100 | train: 0.939923593458, test: 0.958826148361
step: 8300
NDCG@100 | train: 0.892862383438, test: 0.860004172994
step: 8400
NDCG@100 | train: 0.923341339711, test: 0.898228626981
step: 8500
NDCG@100 | train: 0.86782089512, test: 0.85940177989
step: 8600
NDCG@100 | train: 0.951890532448, test: 0.898732401924
step: 8700
NDCG@100 | train: 0.953095921685, test: 0.996728415528
step: 8800
NDCG@100 | train: 0.920319591417, test: 0.87627800122
step: 8900
NDCG@100 | train: 0.928839524826, test: 0.873254890843
step: 9000
NDCG@100 | train: 0.954115074294, test: 0.99150925781
step: 9100
NDCG@100 | train: 0.893306474471, test: 0.957378387792
step: 9200
NDCG@100 | train: 0.948727694716, test: 0.983048020454
step: 9300
NDCG@100 | train: 0.949608963273, test: 0.982119237391
step: 9400
NDCG@100 | train: 0.944417684903, test: 0.918774447942
step: 9500
NDCG@100 | train: 0.939980881512, test: 0.912316097266
step: 9600
NDCG@100 | train: 0.944266696134, test: 0.961243561414
step: 9700
NDCG@100 | train: 0.940511358894, test: 0.983257968798
step: 9800
NDCG@100 | train: 0.931258717595, test: 0.942522244119
step: 9900
NDCG@100 | train: 0.902863374135, test: 0.986196672137
step: 10000
NDCG@100 | train: 0.946330646503, test: 0.979456437747
save the model
save the optimizer
6
6
6
6
6
6
6
6
6
6
4
4
4
4
4
4
4
4
4
4
4
1
1
1
1
1
1
1
1
1
1
1
1
3
3
3
3
3
3
3
3
3
8
8
8
8
8
8
8
8
8
8
8
8
7
7
7
7
7
7
7
7
7
7
7
7
2
2
2
2
2
2
2
2
2
2
2
2
2
5
5
5
5
5
5
5
5
5
5
5
5
5
(680, 680)
load dataset
('The number of data, train:', 646, 'validate:', 34)
prepare initialized model!
step: 100
NDCG@100 | train: 0.385738395835, test: 0.771995470663
step: 200
NDCG@100 | train: 0.63438097837, test: 0.899622055507
step: 300
NDCG@100 | train: 0.70124793336, test: 0.949216417678
step: 400
NDCG@100 | train: 0.691655988421, test: 0.92665828934
step: 500
NDCG@100 | train: 0.661535421378, test: 0.968907066134
step: 600
NDCG@100 | train: 0.792969718282, test: 0.929926861711
step: 700
NDCG@100 | train: 0.807205272438, test: 0.988390826469
step: 800
NDCG@100 | train: 0.771285205798, test: 0.978725843435
step: 900
NDCG@100 | train: 0.394420876068, test: 0.802146469134
step: 1000
NDCG@100 | train: 0.744490386274, test: 0.925381034025
step: 1100
NDCG@100 | train: 0.699134076738, test: 0.939391485583
step: 1200
NDCG@100 | train: 0.748995995414, test: 0.870491015453
step: 1300
NDCG@100 | train: 0.270548262346, test: 0.802146469134
step: 1400
NDCG@100 | train: 0.576282654905, test: 0.866756366744
step: 1500
NDCG@100 | train: 0.817651206198, test: 0.961823218075
step: 1600
NDCG@100 | train: 0.839490851233, test: 0.968985563884
step: 1700
NDCG@100 | train: 0.840586955218, test: 0.987454577044
step: 1800
NDCG@100 | train: 0.844479420053, test: 0.869239270783
step: 1900
NDCG@100 | train: 0.881361223619, test: 0.987136957424
step: 2000
NDCG@100 | train: 0.827908260025, test: 0.933957643916
step: 2100
NDCG@100 | train: 0.861541270607, test: 0.976968573332
step: 2200
NDCG@100 | train: 0.881389141918, test: 0.99604719887
step: 2300
NDCG@100 | train: 0.844454830157, test: 0.965786446435
step: 2400
NDCG@100 | train: 0.87621889239, test: 0.955326730205
step: 2500
NDCG@100 | train: 0.900744091761, test: 0.975757270336
step: 2600
NDCG@100 | train: 0.908292936707, test: 0.975519153487
step: 2700
NDCG@100 | train: 0.861225158954, test: 0.992943368089
step: 2800
NDCG@100 | train: 0.855711231618, test: 0.995301674711
step: 2900
NDCG@100 | train: 0.822296879082, test: 0.969649941211
step: 3000
NDCG@100 | train: 0.861138421838, test: 0.992922253648
step: 3100
NDCG@100 | train: 0.780926316858, test: 0.985215977918
step: 3200
NDCG@100 | train: 0.806048109109, test: 0.992957988145
step: 3300
NDCG@100 | train: 0.771872140926, test: 0.951840844168
step: 3400
NDCG@100 | train: 0.899264737268, test: 0.996865601815
step: 3500
NDCG@100 | train: 0.862607529955, test: 0.989301233873
step: 3600
NDCG@100 | train: 0.899203392593, test: 0.996730096975
step: 3700
NDCG@100 | train: 0.915117878362, test: 0.996572570691
step: 3800
NDCG@100 | train: 0.85603954324, test: 0.989570396156
step: 3900
NDCG@100 | train: 0.902174748933, test: 0.992597005575
step: 4000
NDCG@100 | train: 0.872489571265, test: 0.994547051953
step: 4100
NDCG@100 | train: 0.897059213731, test: 0.995087305049
step: 4200
NDCG@100 | train: 0.885151170999, test: 0.990598880854
step: 4300
NDCG@100 | train: 0.857335765692, test: 0.987003653205
step: 4400
NDCG@100 | train: 0.816447944904, test: 0.982227584657
step: 4500
NDCG@100 | train: 0.850143904294, test: 0.976591193592
step: 4600
NDCG@100 | train: 0.839848678185, test: 0.98945051198
step: 4700
NDCG@100 | train: 0.900421145009, test: 0.989367568533
step: 4800
NDCG@100 | train: 0.882260054493, test: 0.982033521248
step: 4900
NDCG@100 | train: 0.906727483209, test: 0.988771014401
step: 5000
NDCG@100 | train: 0.864079244404, test: 0.985887097211
step: 5100
NDCG@100 | train: 0.896790205918, test: 0.994337645698
step: 5200
NDCG@100 | train: 0.903156229304, test: 0.992248193336
step: 5300
NDCG@100 | train: 0.88188889246, test: 0.992943659721
step: 5400
NDCG@100 | train: 0.902775794554, test: 0.985523359716
step: 5500
NDCG@100 | train: 0.888095956347, test: 0.990333321855
step: 5600
NDCG@100 | train: 0.90787424327, test: 0.989392713313
step: 5700
NDCG@100 | train: 0.91444962325, test: 0.987848130645
step: 5800
NDCG@100 | train: 0.896758173058, test: 0.981862841573
step: 5900
NDCG@100 | train: 0.954051421188, test: 0.995824001129
step: 6000
NDCG@100 | train: 0.952548233275, test: 0.996310038473
step: 6100
NDCG@100 | train: 0.962726764852, test: 0.994653739695
step: 6200
NDCG@100 | train: 0.930813776514, test: 0.992386588416
step: 6300
NDCG@100 | train: 0.937400811965, test: 0.986987195872
step: 6400
NDCG@100 | train: 0.937054678119, test: 0.99257426165
step: 6500
NDCG@100 | train: 0.903011097379, test: 0.995593555595
step: 6600
NDCG@100 | train: 0.920595613764, test: 0.995224767504
step: 6700
NDCG@100 | train: 0.925634236009, test: 0.989877595883
step: 6800
NDCG@100 | train: 0.93683700063, test: 0.991232084746
step: 6900
NDCG@100 | train: 0.926350238581, test: 0.991149976314
step: 7000
NDCG@100 | train: 0.922864136675, test: 0.989961117883
step: 7100
NDCG@100 | train: 0.92154250793, test: 0.985052112859
step: 7200
NDCG@100 | train: 0.927431465978, test: 0.988036024038
step: 7300
NDCG@100 | train: 0.918319612648, test: 0.982289784922
step: 7400
NDCG@100 | train: 0.921405892516, test: 0.975063273551
step: 7500
NDCG@100 | train: 0.930602343438, test: 0.995268752656
step: 7600
NDCG@100 | train: 0.921955238735, test: 0.993207185285
step: 7700
NDCG@100 | train: 0.949662670055, test: 0.99642371203
step: 7800
NDCG@100 | train: 0.921051068094, test: 0.988907286077
step: 7900
NDCG@100 | train: 0.946868667651, test: 0.995488563889
step: 8000
NDCG@100 | train: 0.953776621248, test: 0.995015011465
step: 8100
NDCG@100 | train: 0.937920709376, test: 0.989975138353
step: 8200
NDCG@100 | train: 0.890269162976, test: 0.973163281141
step: 8300
NDCG@100 | train: 0.858681249053, test: 0.982121373892
step: 8400
NDCG@100 | train: 0.883909738718, test: 0.975469793338
step: 8500
NDCG@100 | train: 0.918462750703, test: 0.988880918032
step: 8600
NDCG@100 | train: 0.892013201967, test: 0.97120655927
step: 8700
NDCG@100 | train: 0.902842690746, test: 0.982794793806
step: 8800
NDCG@100 | train: 0.927660713223, test: 0.993960528035
step: 8900
NDCG@100 | train: 0.924496640983, test: 0.983054412137
step: 9000
NDCG@100 | train: 0.912832101952, test: 0.981712071678
step: 9100
NDCG@100 | train: 0.920356369719, test: 0.986691065034
step: 9200
NDCG@100 | train: 0.852237215717, test: 0.920560953186
step: 9300
NDCG@100 | train: 0.831413373855, test: 0.916997849347
step: 9400
NDCG@100 | train: 0.932783656231, test: 0.982919687477
step: 9500
NDCG@100 | train: 0.890562346371, test: 0.977271669363
step: 9600
NDCG@100 | train: 0.915100152188, test: 0.97695998489
step: 9700
NDCG@100 | train: 0.944085397067, test: 0.984020165218
step: 9800
NDCG@100 | train: 0.948983432871, test: 0.991723699009
step: 9900
NDCG@100 | train: 0.936789276472, test: 0.976005909164
step: 10000
NDCG@100 | train: 0.942505663313, test: 0.987996005045
save the model
save the optimizer